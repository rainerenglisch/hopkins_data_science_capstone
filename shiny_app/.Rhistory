#result = n_grams[[3]][frequency == freq & gram_1 == history[1] & gram_2 == history[2], .N]#sum(frequency)]#.N]
result = n_grams[[3]][.( freq, history[1], history[2]), .N, on=c("frequency","gram_1","gram_2"), verbose=verbose]
} else {
#result = n_grams[[3]][frequency >= freq & gram_1 == history[1] & gram_2 == history[2], .N]#sum(frequency)]#.N]
count_all = n_grams[[3]][.(history[1],history[2]), .N,on=c("gram_1","gram_2"), verbose=verbose]
less_freq = n_grams[[3]][.(1:(freq-1),history[1],history[2]), .N,on=c("frequency","gram_1","gram_2"), verbose=verbose]
result = count_all - less_freq
}
} else if (len_hist==3) {
if (comparison == "equals") {
#result = n_grams[[4]][frequency == freq & gram_1 == history[1] & gram_2 == history[2] & gram_3 == history[3], .N]#sum(frequency)]#.N]
result = n_grams[[4]][.( freq, history[1], history[2], history[3]), .N, on=c("frequency","gram_1","gram_2","gram_3"), verbose=verbose]
} else {
#result = n_grams[[4]][frequency >= freq & gram_1 == history[1] & gram_2 == history[2] & gram_3 == history[3], .N]#sum(frequency)]#.N]
count_all = n_grams[[4]][.(history[1],history[2],history[3]), .N,on=c("gram_1","gram_2","gram_3"), verbose=verbose]
less_freq = n_grams[[4]][.(1:(freq-1),history[1],history[2],history[3]), .N,on=c("frequency","gram_1","gram_2","gram_3"), verbose=verbose]
result = count_all - less_freq
}
}
if (is.na(result)) {
result = 0
}
return (result+1)
}
lambda = function(history) {
l = D1*N_c(1,history)
l = l + D2*N_c(2,history)
l = l + D3*N_c(3,history, comparison="greater")
l = l/count_continuation(history)
return (l)
}
p_kn = function(tokens, smoothing = FALSE) {
count_tokens = count(tokens)
tokens_history = tokens[1:length(tokens)-1]
if (!smoothing) {
result = count_tokens - D_c(count_tokens)
result = result / count_continuation(tokens_history)
} else {
#print("Smoothing!")
count_prec = N_c_preceding(tokens)
total_grams=N_c_around(tokens_history)
result = max(count_prec - D_c(count_tokens),0) / total_grams
}
#print(paste0(toString(tokens),": ",result))
# call recursive if at least one token would be left
if (length(tokens) > 2) {
recursive_result = lambda(tokens_history)
#print(paste0("lambda: ",recursive_result))
recursive_result = recursive_result * p_kn(tokens[2:length(tokens)],smoothing=smoothing)
result = result + recursive_result
}
return (result)
}
evaluate = function(history_tokens,follow_tokens) {
for (t in follow_tokens) {
seq = c(history_tokens, t)
print(seq)
print(paste0("prob: ",p_kn(seq, smoothing=FALSE)))
print(paste0("prob(smoothing): ",p_kn(seq, smoothing=TRUE)))
}
}
history_tokens = c("live", "and","i'd")
follow_tokens = c("give","sleep","die","eat")
evaluate(history_tokens,follow_tokens)
history_tokens = c("me", "about","his")
follow_tokens = c("marital","financial","horticultural","spiritual")
evaluate(history_tokens,follow_tokens)
history_tokens = c("arctic", "monkeys","this")
follow_tokens = c("morning","month","weekend","decade")
evaluate(history_tokens,follow_tokens)
history_tokens = c("helps", "reduce","your")
follow_tokens = c("happiness","hunger","stress","sleepiness")
evaluate(history_tokens,follow_tokens)
history_tokens = c("to", "take","a")
follow_tokens = c("minute","look","picture","walk")
evaluate(history_tokens,follow_tokens)
history_tokens = c("to","settle","the")
follow_tokens = c("case","matter","incident","account")
evaluate(history_tokens,follow_tokens)
history_tokens = c("groceries","in","each")
follow_tokens = c("arm","finger","toe","hand")
evaluate(history_tokens,follow_tokens)
history_tokens = c("bottom","to","the")
follow_tokens = c("middle","center","side","top")
evaluate(history_tokens,follow_tokens)
history_tokens = c("bruises","from","playing")
follow_tokens = c("weekly","outside","inside","daily")
evaluate(history_tokens,follow_tokens)
history_tokens = c("of","adam","sandler's")
follow_tokens = c("movies","pictures","novels","stories")
evaluate(history_tokens,follow_tokens)
#------------------------
doc_test_all = readRDS(FNAMES_CORPUS_TEST)
#to do calculate perplexity on train set and test set
probability_sentence = function(sentence, max_hist=4, smoothing=FALSE) {
prob_list = c()
for (i in 1:length(tok_sent)) {
token_seq = tok_sent[max(i-max_hist+1,1):i]
print(token_seq)
p_i = p_kn(token_seq, smoothing=smoothing)
prob_list = c(prob_list,p_i)
print(paste0(i,": ",p_i))
}
print(prob_list)
return (prod(prob_list))
}
probability_corpus = function(corpus, max_hist=4, smoothing=FALSE) {
prob_list = c()
# iterate sentences
for (sentence in corpus) {
print(sentence)
# tokenize sentence
tok_sent = tokens(sentence, padding= T,
remove_punct =T,
remove_symbols = T,
remove_numbers = T,
remove_url = T)[[1]]
print(paste0("tok_sent: ",toString(tok_sent)))
p_i = probability_sentence(tok_sent, max_hist, smoothing)
prob_list = c(prob_list,p_i)
}
print(prob_list)
#return (prod(prob_list))
return (sum(log2(prob_list)))
}
cross_entropy_corpus = function(corpus, max_hist=4, smoothing=FALSE) {
prob_corpus = probability_corpus(corpus, max_hist=max_hist, smoothing=smoothing)
corpus_token_length = length(tokens(corpus, padding= T,
remove_punct =T,
remove_symbols = T,
remove_numbers = T,
remove_url = T))
#entropy = - log2(prob_corpus) / corpus_token_length
entropy = - (prob_corpus) / corpus_token_length
print(paste0("entropy: ",entropy))
return (entropy)
}
perplexity_corpus = function(corpus, max_hist=4, smoothing=FALSE) {
entropy = cross_entropy_corpus(corpus, max_hist, smoothing)
perplexity = `^`(2,entropy)
print(paste0("perplexity: ",perplexity))
return (perplexity)
}
perplexity_corpus(doc_test_all[1:10])
perplexity_corpus(doc_test_all[1:10],max_hist=2)
perplexity_corpus(doc_test_all[1:10],max_hist=3)
perplexity_corpus(doc_test_all[1:1000],max_hist=3)
perplexity_corpus(doc_test_all[1:10000],max_hist=3)
probability_corpus = function(corpus, max_hist=4, smoothing=FALSE) {
#prob_list = c()
prob = 0
# iterate sentences
for (sentence in corpus) {
print(sentence)
# tokenize sentence
tok_sent = tokens(sentence, padding= T,
remove_punct =T,
remove_symbols = T,
remove_numbers = T,
remove_url = T)[[1]]
print(paste0("tok_sent: ",toString(tok_sent)))
p_i = probability_sentence(tok_sent, max_hist, smoothing)
#prob_list = c(prob_list,p_i)
prob = prob + log2(p_i)
}
#print(prob_list)
print(prob)
#return (prod(prob_list))
#return (sum(log2(prob_list)))#
return (prob)
}
probability_corpus = function(corpus, max_hist=4, smoothing=FALSE) {
#prob_list = c()
prob = 0
# iterate sentences
for (sentence in corpus) {
print(sentence)
# tokenize sentence
tok_sent = tokens(sentence, padding= T,
remove_punct =T,
remove_symbols = T,
remove_numbers = T,
remove_url = T)[[1]]
print(paste0("tok_sent: ",toString(tok_sent)))
p_i = probability_sentence(tok_sent, max_hist, smoothing)
#prob_list = c(prob_list,p_i)
prob = prob + log2(p_i)
}
#print(prob_list)
print(prob)
#return (prod(prob_list))
#return (sum(log2(prob_list)))#
return (prob)
}
perplexity_corpus(doc_test_all[1:1000])#,max_hist=3)
perplexity_corpus(doc_test_all[1:10])#,max_hist=3)
install.packages("shiny")
shiny::runApp('shiny_app')
shiny::runApp('shiny_app')
runApp('shiny_app')
runApp('shiny_app')
runApp('shiny_app')
runApp('shiny_app')
runApp()
runApp('shiny_app')
runApp('shiny_app')
textInput = "i am grateful"
text_split = str_split(str_trim(textInput), pattern=" ", simplify = TRUE)
library(shiny)
library(stringr)
text_split = str_split(str_trim(textInput), pattern=" ", simplify = TRUE)
result = n_grams[[4]][text_split[(length(text_split)-3):length(text_split)],p_kn, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][text_split[(length(text_split)-2:length(text_split)],p_kn, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][text_split[(length(text_split)-2):length(text_split)],p_kn, on = c("gram_1","gram_2","gram_3")]
n_grams[[4]]
source("./use_n_grams_probs.R")
setwd("~/dev/hopkins_ds_capstone/shiny_app")
source("./use_n_grams_probs.R")
result = n_grams[[4]][text_split[(length(text_split)-3):length(text_split)],p_kn, on = c("gram_1","gram_2","gram_3","gram_4")]
result = n_grams[[4]][text_split[(length(text_split)-3):length(text_split)],p_kn, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][text_split[(length(text_split)-2):length(text_split)],p_kn, on = c("gram_1","gram_2","gram_3")]
text_split[(length(text_split)-2):length(text_split)]
result = n_grams[[4]][text_split[(length(text_split)-2):length(text_split)],p_kn, on = c("gram_1","gram_2","gram_3")]
runApp()
typeof(text_split[(length(text_split)-2):length(text_split)])
text_split[(length(text_split)-2):length(text_split)]
result = n_grams[[4]][text_split[(length(text_split)-2):length(text_split)],p_kn, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][text_split[c("i","am","grateful"),p_kn, on = c("gram_1","gram_2","gram_3")]
]
result = n_grams[[4]][c("i","am","grateful"),p_kn, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][c("i","am","grateful","for"),p_kn, on = c("gram_1","gram_2","gram_3","gram_4")]
result = n_grams[[4]][c("i","am","grateful","for"),p_kn, on = c("gram_1","gram_2","gram_3","gram_4"),verbose=TRUE]
result = n_grams[[4]][.(c("i","am","grateful","for")),p_kn, on = c("gram_1","gram_2","gram_3","gram_4"),verbose=TRUE]
BASE_DIR2="./"
FNAMES_N_GRAMS = paste0(BASE_DIR2,c("freq_1_grams.Rds","freq_2_grams.Rds","freq_3_grams.Rds","freq_4_grams.Rds"))
FNAMES_N_GRAMS_PROBS = paste0(BASE_DIR2,c("freq_1_grams_probs.Rds","freq_2_grams_probs.Rds","freq_3_grams_probs.Rds","freq_4_grams_probs.Rds"))
n_grams = list()
for (n_i in 1:4) {
n_grams=append(n_grams, list(readRDS(FNAMES_N_GRAMS[n_i])))
}
# setting indizes for n_grams
setindex(n_grams[[1]], gram_1)
setindex(n_grams[[1]], frequency)
indices(n_grams[[1]])
setindex(n_grams[[2]], gram_1, gram_2)
setindex(n_grams[[2]], gram_1)
setindex(n_grams[[2]], frequency)
setindex(n_grams[[2]], frequency, gram_1)
setindex(n_grams[[2]], gram_2)
indices(n_grams[[2]])
setindex(n_grams[[3]], gram_1, gram_2, gram_3)
setindex(n_grams[[3]], gram_1, gram_2)
setindex(n_grams[[3]], gram_2)
setindex(n_grams[[3]], frequency)
setindex(n_grams[[3]], frequency, gram_1, gram_2)
setindex(n_grams[[3]], gram_2, gram_3)
indices(n_grams[[3]])
setindex(n_grams[[4]], gram_1, gram_2, gram_3, gram_4)
setindex(n_grams[[4]], gram_1, gram_2, gram_3)
setindex(n_grams[[4]], gram_2, gram_3)
setindex(n_grams[[4]], frequency)
setindex(n_grams[[4]], frequency, gram_1, gram_2, gram_3)
setindex(n_grams[[4]], gram_2, gram_3, gram_4)
indices(n_grams[[4]])
BASE_DIR2="./"
FNAMES_N_GRAMS_PROBS = paste0(BASE_DIR2,c("freq_1_grams_probs.Rds","freq_2_grams_probs.Rds","freq_3_grams_probs.Rds","freq_4_grams_probs.Rds"))
n_grams = list()
for (n_i in 1:4) {
n_grams=append(n_grams, list(readRDS(FNAMES_N_GRAMS_PROBS[n_i])))
}
result = n_grams[[4]][.(c("i","am","grateful","for")),p_kn, on = c("gram_1","gram_2","gram_3","gram_4"),verbose=TRUE]
result = n_grams[[4]][.("i","am","grateful","for"),p_kn, on = c("gram_1","gram_2","gram_3","gram_4"),verbose=TRUE]
result = n_grams[[1]][text_split[length(text_split)],p_kn, on = "gram_1"]
text_split = str_split(str_trim(input$textInput), pattern=" ")#, simplify = TRUE)
text_split = str_split(str_trim(textInput), pattern=" ")#, simplify = TRUE)
result = n_grams[[1]][text_split[length(text_split)], p_kn, on = "gram_1"]
text_split[length(text_split)]
text_split = str_split(str_trim(input$textInput), pattern=" ")#, simplify = TRUE)
result = n_grams[[1]][text_split[length(text_split)], p_kn, on = "gram_1"]
runApp()
n_grams[[1]][,p_kn]
n_grams[[1]][p_kn]
n_grams[[1]]
n_grams[[1]][,.N]
n_grams[[1]][,.N,]
n_grams[[1]][.N]
n_gram[[1]][frequency == 1,sum(frequency)]
n_grams[[1]][frequency == 1,sum(frequency)]
n_gram_max =n_grams[[length(n_grams)]]
n1 = n_gram_max[frequency == 1,sum(frequency)]#.N]
n2 = n_gram_max[frequency == 2,sum(frequency)]#.N]
n3 = n_gram_max[frequency == 3,sum(frequency)]#.N]
n4 = n_gram_max[frequency == 4,sum(frequency)]#.N]
Y = n1/(n1+2*n2)
n_gram_max =n_grams[[length(n_grams)]]
n1 = n_gram_max[frequency == 1,sum(frequency)]#.N]
n2 = n_gram_max[frequency == 2,sum(frequency)]#.N]
n3 = n_gram_max[frequency == 3,sum(frequency)]#.N]
n4 = n_gram_max[frequency == 4,sum(frequency)]#.N]
Y = n1/(n1+2*n2)
#
# calculate Ds
n_gram_max =n_grams[[length(n_grams)]]
n1 = n_gram_max[frequency == 1,sum(frequency)]#.N]
typeof(n_grams[[1]])
typeof(n_grams[[1]][1])
typeof(n_grams[[1]][2])
typeof(n_grams[1])
typeof(n_grams[2])
typeof(n_grams[4])
typeof(n_grams[5])
typeof(n_grams[[5])
typeof(n_grams[[4])
typeof(n_grams[[4]])
typeof(n_grams[[5]])
typeof(n_grams[[4]])
typeof(n_grams[[4]][1])
typeof(n_grams[[4]][10])
n1 = n_gram_max[frequency == 1,sum(frequency)]#.N]
n1 = n_gram_max[frequency == 1,sum(frequency)]
r
typeof(n_gram_max)
BASE_DIR2="./"
FNAMES_N_GRAMS_PROBS = paste0(BASE_DIR2,c("freq_1_grams_probs.Rds","freq_2_grams_probs.Rds","freq_3_grams_probs.Rds","freq_4_grams_probs.Rds"))
n_grams = list()
for (n_i in 1:4) {
n_grams=append(n_grams, list(readRDS(FNAMES_N_GRAMS_PROBS[n_i])))
}
typeof(n_grams[[1]])
typeof(n_grams[[4]])
n_grams[[4]]$feature
FNAMES_N_GRAMS_PROBS = paste0(BASE_DIR2,c("freq_1_grams_probs.Rds","freq_2_grams_probs.Rds","freq_3_grams_probs.Rds","freq_4_grams_probs.Rds"))
n_grams = list()
for (n_i in 1:4) {
n_grams=append(n_grams, readRDS(FNAMES_N_GRAMS_PROBS[n_i]))
}
n_grams = list()
for (n_i in 1:4) {
n_grams=append(n_grams, list(readRDS(FNAMES_N_GRAMS_PROBS[n_i])))
}
View(n_grams)
typeof(n_grams[[1]])
library(data.table)
result = n_grams[[1]][text_split[length(text_split)], p_kn, on = "gram_1"]
runApp()
textInput = "you are nice"
text_split = str_split(str_trim(input$textInput), pattern=" ", simplify = TRUE)
text_split = str_split(str_trim(textInput), pattern=" ", simplify = TRUE)
result = n_grams[[4]][text_split[(length(text_split)-3):length(text_split)],p_kn, on = c("gram_1","gram_2","gram_3","gram_4")]
result = n_grams[[4]][text_split[(length(text_split)-3):length(text_split)],p_kn, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][text_split[(length(text_split)-2):length(text_split)],p_kn, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[1]][text_split[length(text_split)], p_kn, on = "gram_1"]
text_split[(length(text_split)-2):length(text_split)]
> result = n_grams[[4]][text_split[(length(text_split)-2):length(text_split)],p_kn, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][text_split[(length(text_split)-3):length(text_split)],p_kn, on = c("gram_1","gram_2","gram_3","gram_4")]
result = n_grams[[4]][text_split[(length(text_split)-3):length(text_split)],gram_4, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][text_split[(length(text_split)-3):length(text_split)],gram_4, on = c("gram_1","gram_2","gram_3")]
# setting indizes for n_grams
setindex(n_grams[[1]], gram_1)
setindex(n_grams[[1]], frequency)
indices(n_grams[[1]])
setindex(n_grams[[2]], gram_1, gram_2)
setindex(n_grams[[2]], gram_1)
setindex(n_grams[[2]], frequency)
setindex(n_grams[[2]], frequency, gram_1)
setindex(n_grams[[2]], gram_2)
indices(n_grams[[2]])
setindex(n_grams[[3]], gram_1, gram_2, gram_3)
setindex(n_grams[[3]], gram_1, gram_2)
setindex(n_grams[[3]], gram_2)
setindex(n_grams[[3]], frequency)
setindex(n_grams[[3]], frequency, gram_1, gram_2)
setindex(n_grams[[3]], gram_2, gram_3)
indices(n_grams[[3]])
setindex(n_grams[[4]], gram_1, gram_2, gram_3, gram_4)
setindex(n_grams[[4]], gram_1, gram_2, gram_3)
setindex(n_grams[[4]], gram_2, gram_3)
setindex(n_grams[[4]], frequency)
setindex(n_grams[[4]], frequency, gram_1, gram_2, gram_3)
setindex(n_grams[[4]], gram_2, gram_3, gram_4)
indices(n_grams[[4]])
result = n_grams[[4]][text_split[(length(text_split)-3):length(text_split)],gram_4, on = c("gram_1","gram_2","gram_3")]
setindex(n_grams[[4]], gram_1, gram_2, gram_3)
indices(n_grams[[4]])
result = n_grams[[4]][text_split[(length(text_split)-3):length(text_split)],gram_4, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][.("i","am","here"),gram_4, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][.("i","am","here"),.("gram_4","p_kn"), on = c("gram_1","gram_2","gram_3")]
result
result = n_grams[[4]][.("i","am","here"),c("gram_4","p_kn"), on = c("gram_1","gram_2","gram_3")]
result
result = n_grams[[4]][c("i","am","here"),c("gram_4","p_kn"), on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][.text_split[(length(text_split)-3):length(text_split)],gram_4, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][.(text_split[(length(text_split)-3):length(text_split)]),gram_4, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][ (text_split[(length(text_split)-3):length(text_split)]),gram_4, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][ c(text_split[(length(text_split)-3):length(text_split)]),gram_4, on = c("gram_1","gram_2","gram_3")]
text_split[(length(text_split)-3):length(text_split)]
typeof(text_split[(length(text_split)-3):length(text_split)])
c("you","are","nice")
typeof(c("you","are","nice"))
result = n_grams[[4]][ text_split[(length(text_split)-3):length(text_split)],gram_4, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][ list(text_split[(length(text_split)-3):length(text_split)]),gram_4, on = c("gram_1","gram_2","gram_3")]
list(text_split[(length(text_split)-3):length(text_split)])
c(text_split[(length(text_split)-3):length(text_split)])
text_split[(length(text_split)-3):length(text_split)]
result = n_grams[[4]][c(text_split[(length(text_split)-3):length(text_split)]),gram_4, on = c("gram_1","gram_2","gram_3")]
text_query = text_split[(length(text_split)-3):length(text_split)]
result = n_grams[[4]][text_query,gram_4, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][ as.integer(text_query),gram_4, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][ as.integer(text_query,gram_4, on = c("gram_1","gram_2","gram_3")]
as.vector()
result = n_grams[[4]][ as.vector(text_query,gram_4, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][ as.vector(text_query),gram_4, on = c("gram_1","gram_2","gram_3")]
result = n_grams[[1]][text_split[length(text_split)], p_kn, on = "gram_1"]
text_split[length(text_split)]
result = n_grams[[1]][text_split[length(text_split)], p_kn, on = c("gram_1")]
result
result = n_grams[[1]][c(text_split[length(text_split)]), p_kn, on = c("gram_1")]
result = n_grams[[1]][c(text_split[(length(text_split)-1):length(text_split)]), p_kn, on = c("gram_1")]
result = n_grams[[2]][c(text_split[(length(text_split)-1):length(text_split)]), p_kn, on = c("gram_1","gram_2")]
result = n_grams[[2]][.(text_split[(length(text_split)-1):length(text_split)]), p_kn, on = c("gram_1","gram_2")]
result = n_grams[[2]][list(text_split[(length(text_split)-1):length(text_split)]), p_kn, on = c("gram_1","gram_2")]
result = n_grams[[2]][list(text_split[(length(text_split)-1):length(text_split)]), p_kn, on = c("gram_1")]
result = n_grams[[2]][list(list(text_split[(length(text_split)-1):length(text_split)])), p_kn, on = c("gram_1")]
result = n_grams[[2]][list(list(text_split[(length(text_split)-1):length(text_split)])), p_kn, on = c("gram_1","gram_")]
result = n_grams[[2]][list(list(text_split[(length(text_split)-1):length(text_split)])), p_kn, on = c("gram_1","gram_2")]
result = n_grams[[2]][list(list(text_split[(length(text_split)-1):length(text_split)])),  on = c("gram_1","gram_2")]
result = n_grams[[2]][text_split[(length(text_split)-1):length(text_split)]),  on = c("gram_1","gram_2")]
result = n_grams[[2]][text_split[(length(text_split)-1):length(text_split)],  on = c("gram_1","gram_2")]
result = n_grams[[2]][.(text_split[(length(text_split)-1):length(text_split)]),  on = c("gram_1","gram_2")]
result = n_grams[[2]][.(text_split[(length(text_split)-1):length(text_split)]),  on = c("gram_1")]
result = n_grams[[2]][.(text_split[(length(text_split)-1):length(text_split)]),  on = c("gram_1")]
text_split[(length(text_split)-1):length(text_split)]
typeof(text_split[(length(text_split)-1):length(text_split)])
result = n_grams[[2]][as.list(text_split[(length(text_split)-1):length(text_split)]),  on = c("gram_1")]
result = n_grams[[2]][as.list(text_split[(length(text_split)-1):length(text_split)]),  on = c("gram_1","gram_2")]
result = n_grams[[4]][as.list(text_split[(length(text_split)-3):length(text_split)]),c("gram_4","p_kn"), on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][as.list(text_split[(length(text_split)-2):length(text_split)]),c("gram_4","p_kn"), on = c("gram_1","gram_2","gram_3")]
result
text_split[(length(text_split)-2):length(text_split)]
runApp()
n_grams[[4]][1:5]
n_grams[[4]][5:50]
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
text_split[(length(text_split)-3):length(text_split)]
text_split = str_split(str_trim("1st of the month"), pattern=" ", simplify = TRUE)
text_split[(length(text_split)-3):length(text_split)]
text_split[(length(text_split)-2):length(text_split)]
runApp()
result = n_grams[[4]][as.list(text_split[(length(text_split)-2):length(text_split)]),c("gram_4","p_kn"), on = c("gram_1","gram_2","gram_3")]
result
text_split = str_split(str_trim("i were refugee in mexico", pattern=" ", simplify = TRUE)
)
text_split = str_split(str_trim("i were refugee in mexico"), pattern=" ", simplify = TRUE)
result = n_grams[[4]][as.list(text_split[(length(text_split)-2):length(text_split)]),c("gram_4","p_kn"), on = c("gram_1","gram_2","gram_3")]
result
nrow((result))
nrow(result)
result$p_kn
result$p_kn[1]
result$p_kn[[1]
]
is.na(result$p_kn[1])
runApp()
result
result[,1]
result[,1]
result[,2]
result[,3]
result[,1]
runApp()
result = n_grams[[4]][as.list(text_split[(length(text_split)-2):length(text_split)]),c("gram_4","p_kn"), on = c("gram_1","gram_2","gram_3")]
result = n_grams[[4]][as.list(text_split[(length(text_split)-2):length(text_split)]),c("gram_4","p_kn"), on = c("gram_1","gram_2","gram_3")]
result
text_split = str_split(str_trim("1st of the"), pattern=" ", simplify = TRUE)
result
result = n_grams[[4]][as.list(text_split[(length(text_split)-2):length(text_split)]),c("gram_4","p_kn"), on = c("gram_1","gram_2","gram_3")]
result
result[1,1]
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
result
result$gram_4
result[1]
result$gram_4[[1]]
result$gram_4
typeof(result$gram_4)
typeof(result$gram_4)
result$gram_4
as.string(result$gram_4)
asstring(result$gram_4)
asString(result$gram_4)
as.character(result$gram_4)
typeof(as.character(result$gram_4))
runApp()
as.character(result[1,1])
as.character(result[1,1][[1]])
as.character(result[1,1][1])
runApp()
result
runApp()
result
runApp()
