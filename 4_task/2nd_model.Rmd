---
title: "R Notebook"
output: html_notebook
---

```{r loading_libraries, echo=FALSE}
library("quanteda")
library("quanteda.textplots")
library("quanteda.textstats")
library("dplyr")
require(readtext)
library(ggplot2)
library(data.table)
library(stringr)
library(sqldf)

```
# Summary
In this notebook we will try to build a simple n-gram based language model that predicts the next word based on the n-1 previous words.
1. We will define metrics to evaluate the performance: perplexity and accuracy (at the first, second and third word)-
2. We will try different hyperparameters for the model and assess the size and performance.
3. We will correlate perplexity with others measures of accuracy
4. We will assess wheter it its possible to reduce the size of the model without reducing performance.

## Importing text and creating tokens

```{r load_corpus, cache=TRUE, echo=TRUE}
BASE_DIR = "../data/en_US/"
FNAMES = c("en_US.twitter.txt","en_US.blogs.txt","en_US.news.txt")
FNAME_DT = "./freq_n_grams.Rds"

create_n_gram_model = function(toks, n = 2, n_freq_n_grams=1000000){
    # create two grams
    print(paste0("creating ",n,"ngrams"))
    n_grams = tokens_ngrams(toks, n = n)
    # create two gram dfm 
    print("creating dfm for n_grams")
    dfm_n_grams = dfm(n_grams)
    print("calculating frequencies from dfm")
    freq_n_grams = textstat_frequency(dfm_n_grams,n=n_freq_n_grams)
    grams = sapply(freq_n_grams$feature, str_split, pattern="_")
    # create separate column for each gram
    for (i in 1:n) {
      freq_n_grams = cbind(freq_n_grams, gram = sapply(grams, `[[`, i))
      colnames = colnames(freq_n_grams)[colnames(freq_n_grams) == "gram"] = paste0("gram_", i)
    }
    
    return (freq_n_grams)
}

if (!file.exists(FNAME_DT)) {
  dt_freq_n_grams = NULL
  for (fname in FNAMES) {
    print(paste0("Processing ", fname))
    # read plain text
    raw_doc = readtext(paste0(BASE_DIR,fname))
    # create quanteda corpus
    doc = corpus(raw_doc)
    raw_doc = NULL
    # transform to document per sentence
    doc = corpus_reshape(doc, to ="sentences")
    #doc=corpus_sample(doc, size = 1000)
    toks = tokens(doc, padding= T,
                remove_punct =T,  
                remove_symbols = T,
                remove_numbers = T,
                remove_url = T)
    doc = NULL
    gc()
    temp_dt_freq_n_grams = as.data.table(create_n_gram_model(toks,n=5),stringsAsFactors = FALSE) 
    #, n_freq_n_grams=1000
    temp_dt_freq_n_grams =sqldf("SELECT feature, frequency, docfreq, gram_1, gram_2, gram_3, gram_4 FROM temp_dt_freq_n_grams")
    if (is.null(dt_freq_n_grams)){
      dt_freq_n_grams = temp_dt_freq_n_grams
      }
    else{
      dt_freq_n_grams = rbind(dt_freq_n_grams,temp_dt_freq_n_grams)
      dt_freq_n_grams=sqldf("SELECT feature, SUM(frequency) AS frequency, SUM(docfreq) AS docfreq, min(gram_1) AS gram_1, min(gram_2) as gram_2, min(gram_3) AS gram_3, min(gram_4) as gram_4 FROM dt_freq_n_grams GROUP BY feature")
    }
    temp_dt_freq_n_grams = NULL
    gc()
  }
  # save data frame
  saveRDS(dt_freq_n_grams, file =FNAME_DT)#, compress=FALSE)
} else {
  dt_freq_n_grams=readRDS(FNAME_DT)
}

```
```{r}
# kneyser-ney smoothing constand
d = 0.75
# calculate probability of word sequence 
w = c("I","have","a","dream")

# p continuation
get_count_n_grams = function(n=2) {
  sql_stmt = "SELECT COUNT(DISTINCT gram_1 || '__' || gram_2) FROM dt_freq_n_grams"
  result = sqldf(sql_stmt)
  return (result[1,1])
}

get_count_preceding = function(w) {
  sql_stmt = paste0("SELECT COUNT(DISTINCT gram_1) FROM dt_freq_n_grams WHERE gram_2 = '",w,"'")
  result = sqldf(sql_stmt)
  return (result[1,1])
}

get_for_ngram = function(w, select = "SUM(frequency)") {
  GROUPBY_CLAUSE = ""
  WHERE_CLAUSE = ""
  i = 1
  for (e in w) {
    WHERE_CLAUSE = paste0(WHERE_CLAUSE,"gram_",i," = '",e,"'")
    if (i < length(w)) {
      WHERE_CLAUSE = paste0(WHERE_CLAUSE, " AND ")
    }
    i=i+1
  }
  sql_stmt = paste0("SELECT SUM(frequency) AS frequency FROM dt_freq_n_grams WHERE ", WHERE_CLAUSE)
  print(sql_stmt)
  result = sqldf(sql_stmt)
  return (result[1,1])
}

# counts how often the token occure
C_kn = function(tokens) {
  # if token sequence is greater than one then just count the frequency of the tokens in the ngram table
  count = get_for_ngram(tokens)
  return (count)
}

lambda_kn = function(tokens, d=0.75) {
  l = (d / C_kn(tokens)) * get_for_ngram(tokens, select = "COUNT(*)")
  return (l)
}
# calculate the kneser ney smoothing probability of n-gram
P_kn = function(tokens, d=0.75) {
  print("P_kn")
  print(tokens)
  #token_history = tokens[(length(tokens)-n+1):length(tokens)]
  token_history = tokens[1:length(tokens)-1]
  print(token_history)
  result = max(C_kn(tokens)-d,0)/C_kn(token_history) 
  if (length(token_history) > 1) { 
    result = result + lambda_kn(token_history,d=d) * P_kn(tokens[2:length(tokens)], d=d)
  } else {
    # if token sequence equals one than account for unigram by continuation probability
    result = result + lambda_kn(token_history,d=d) * get_count_preceding(tokens) / get_count_n_grams()
  }
  return (result)
}
w = c("i","have","a","dream")
P_kn(w)
P_kn(c("i","have","a","car"))
```

```{r}
#https://en.wikipedia.org/wiki/Katz's_back-off_model
sqldf("SELECT SUM(frequency), gram_1,gram_2,gram_3,gram_4 FROM dt_freq_n_grams GROUP BY gram_1, gram_2, gram_3 HAVING gram_1='for' and gram_2='the' and gram_3='first'")

```


```{r}

sqldf("select gram_1, SUM(frequency) from dt_freq_n_grams  group by gram_1 having gram_1  IN ('novels','pictures','stories','movies')"  )

sqldf("select gram_1, gram_2, SUM(frequency) from dt_freq_n_grams  group by gram_1,gram_2 having gram_1 = 'i''d' and gram_2 IN ('eat','give','die','sleep')"  )

sqldf("select gram_1, gram_2, gram_3, SUM(frequency) from dt_freq_n_grams  group by gram_1,gram_2, gram_3 having gram_1 = 'and' and  gram_2 = 'i''d' and gram_3 IN ('eat','give','die','sleep')")

```

