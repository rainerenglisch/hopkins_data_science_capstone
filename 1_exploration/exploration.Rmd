---
title: "DS capstone project - Exploratory analysis"
author: "Rainer-Anton Englisch"
date: "11th may 2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exploratory analysis and approach for Data Science capstone project


## Basic summary

I am using the R library quanteda for exploring the text files.

```{r loading_libraries, echo=FALSE}
library("quanteda")
library("quanteda.textplots")
library("quanteda.textstats")
require(readtext)
library(ggplot2)
```

Firstly the documents and sentences are loaded. From each of the three source files (blogs, news and twitter) only 10000 sentences are sampled on analyzed further. This is because to keep the resource requirements of the RMarkdown file low.

```{r load_corpus, cache=TRUE, echo=FALSE}
base_dir = "../data/en_US/"
fnames = c("en_US.blogs.txt"
           ,"en_US.news.txt"
           ,"en_US.twitter.txt"
           )

# loading the raw text files
raw_docs = lapply(paste0(base_dir,fnames), readtext)
# creating corpuses
docs = lapply(raw_docs, corpus)
# reshape corpuses to sentences
docs = lapply(docs, corpus_reshape, to ="sentences")
# sampling for performance
docs = lapply(docs,corpus_sample, size = 10000)

# creating tokens
toks = lapply(docs,tokens)
# cleaning up the raw_docs variable as it takes up too 400MB RAM.
raw_docs = NULL
gc()
```

### Word counts and line counts

```{r counts,cache=TRUE,  fig.align = 'center'}
# count sentences for each text corpus
count_sentences = sapply(toks,ndoc)

# count words for each text corpus
dist_words = lapply(toks, ntoken) 
count_words = sapply(dist_words, sum)

# summarize in one data frame
df_summary = data.frame(file=fnames, 
           count_sentences = count_sentences, 
           count_words = count_words)
print(df_summary)

df_dist_words =data.frame(dist_words)
names(df_dist_words) = fnames

for (c in names(df_dist_words)) {
  hist(df_dist_words[,c], 
       xlab="Tokens in sentence",
       main=paste0(c,": Distribution of tokens in sentences"))
  first_plot=FALSE
}
```

- When we compare the text data from the three sources. We see that the sentences from blogs and news are very similar in distribution of count of words: Most of the sentences have between 10 and 20 words.

- As expected the twitter distribution is quite different as the number of words in tweets are very restricted: The highest bin has up to 20 words per sentence.

### Word cloud, most frequent words
```{r word_frequencies, cache=TRUE, fig.align = 'center'}
# creating tokens and removing punctuation
toks = lapply(docs,tokens,remove_punct =T)
#toks = lapply(toks, tokens_select, pattern=stopwords("en"), selection="remove", padding=FALSE)
# create 
dfmats = lapply(toks,dfm)
dfmats = lapply(dfmats,dfm_remove,pattern=stopwords("en"))
dfmats = lapply(dfmats,dfm_trim, min_termfreq = 50)
top_features = lapply(dfmats, topfeatures,n=10)
i=0
for (tf in top_features) {
  i=i+1
  df = data.frame(tf)
  barplot(height=df$tf, names=row.names(df), 
          horiz=TRUE,, las=1,
          main=paste0("Most frequents words in ",fnames[i]))
  textplot_wordcloud(dfmats[[i]])
}

``` 
Here we look at the occurrences of the words. To reduce the words to be considered in the analysis only words with minimum frequency of 15 are choosen. Additionally, english stop words were filtered out.

If we compare the occurrences of the words we see that:

- In the blogs source we have casual language. However, rather non-emotional: E.g. one, can, know, time.
- In the news source we have as top word "said" to quote some person. Additionally, the word "$" relating to amounts of money (e.g. spending of government). Thus, we see words that represent formal language.
- In twitter, we see words that reflect informal and casual speech. However, the words are a little more "affective"/emotional and related to the current situation: e.g. just, good, like, love, thanks.

## n-grams
```{r n_grams, fig.align = 'center', fig.width=10}
# create two grams

two_grams = lapply(toks,tokens_ngrams)
# create two gram dfm 
dfm_two_grams = lapply(two_grams, dfm)
# calculate frequencies for two grams
freq_two_grams = lapply(dfm_two_grams,textstat_frequency,n =100)
i=0
for (ftg in freq_two_grams) {
  i=i+1
  print(i)
  # Sort by reverse frequency order: tbd
  ftg$feature <- with(ftg, reorder(feature, -frequency))
  p = ggplot(ftg, aes(x = feature, y = frequency)) +
    geom_point() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    ggtitle(paste0(fnames[i],": Frequency of 2 grams"))
  print(p)
}

``` 

```{r}
freq_two_grams = lapply(dfm_two_grams,textstat_frequency,n =1000)
cumsum_freq_two_grams= cumsum(features_two_grams[[1]]$frequency)/sum(features_two_grams[[1]]$frequency)
p = ggplot(ftg, aes(x = feature, y = frequency)) +
    geom_point() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    ggtitle(paste0(fnames[i],": Frequency of 2 grams"))
print(p)
```


## Approach for language model

Goal of the capstone project is to 

- recommend the next word given an ordered list of previous words and
- to autocomplete the current word

My approach is to build and evaluate several probability models that 

- take as input n previous words and 
- outputs for each word of the vocabulary a probability that it is the next word

One simple approach is to create an n-gram based model.


